---
title: "Build a real-time RAG chatbot using Google Drive and Sharepoint"
subtitle: "Keep your chatbot\u2019s knowledge base up-to-date with Pathway and LlamaIndex"
date: 2024-03-07
authors:
  - "Anup Surendran"
category: "LLMs"
---

![Build a real-time RAG chatbot using Google Drive and Sharepoint](https://streamlit.ghost.io/content/images/size/w2000/2024/03/title_streamlit-pathway-llm-google-doc.svg)


In this post, we explore how to build a real-time RAG app with up-to-date information from your files stored in Google Drive or Sharepoint. This means that your chatbot will always have access to the most recent version of your knowledge base‚Äîno manual pipeline reruns needed. By the end of this tutorial, you‚Äôll use Pathway and LlamaIndex to build a RAG chatbot that instantly updates.

## **Why Pathway?**

Pathway is an open data processing framework. It allows you to easily develop data transformation pipelines and ML apps that work with live data sources. Pathway listens to your documents for changes, additions, and removals. It handles loading and indexing without ETL.

Pathway offers an indexing solution that is always up-to-date without the need for traditional ETL pipelines. It can monitor several data sources (such as files, S3 folders, and cloud storage) and provide the latest info to your LLM app.

This means you don‚Äôt need to worry about:

* Checking files to see if there are any changes
* Parsing PDFs, Word documents, or other text files
* Transforming, embedding documents, and loading them into a vector database

Once updates are made to the files that make up your knowledge base, the updated content is immediately re-indexed ‚Äî you don‚Äôt have to deal with rerunning the pipeline.

## **App overview**

This demo consists of three parts.

1. For up-to-date knowledge and information retrieval from the knowledge base‚Äôs documents, Pathway‚Äôs vector store is used.
2. LlamaIndex creates the RAG pipeline and offers chat memory.
3. Streamlit powers the easy-to-navigate user interface.

## **Tutorial: Creating a real-time RAG app with Pathway + LlamaIndex**

üèÇ

Want to jump right in? Check out the [app](https://chat-realtime-sharepoint-gdrive.streamlit.app/?ref=streamlit.ghost.io) to see how it works or explore the code in the [GitHub repository](https://github.com/pathway-labs/realtime-indexer-qa-chat/blob/main/demo/app.py?ref=streamlit.ghost.io).

## Prerequisites

* An OpenAI API key (check out [our instructions](https://streamlit.ghost.io/langchain-tutorial-1-build-an-llm-powered-app-in-18-lines-of-code/#step-1-get-an-openai-api-key) if you don‚Äôt already have one)
* [A Pathway instance](https://pathway.com/solutions/ai-pipelines?ref=streamlit.ghost.io) (the hosted version is provided free for the demo)

## 1. Adding data to the knowledge base

Pathway can listen to many sources simultaneously, such as local files, S3 folders, cloud storage, and data streams. In this example, you‚Äôll add example documents to your pipeline by uploading files to a Google Drive registered to Pathway as a source. You can also check out the [full docs on Pathway‚Äôs Google Drive connector](https://pathway.com/developers/user-guide/connectors/gdrive-connector/?ref=streamlit.ghost.io).

For this demo, a Google Drive folder is provided for you to upload files. To test the app, we‚Äôll ask our assistant questions about Pathway and it will respond based on the available files in the Google Drive folder.

See [pathway-io](https://pathway.com/developers/api-docs/pathway-io?ref=streamlit.ghost.io) for more information on available connectors and how to implement custom connectors.

## 2. Building a Pathway-powered chatbot

### Retriever

First, import the necessary modules for the app.

```
from llama_index.retrievers import PathwayRetriever
from llama_index.query_engine import RetrieverQueryEngine
from llama_index.chat_engine.condense_question import CondenseQuestionChatEngine
from rag import chat_engine
```

Initialize the retriever with the hosted Pathway instance and create the query engine:

```
PATHWAY_HOST = "api-pathway-indexer.staging.deploys.pathway.com"
PATHWAY_PORT = 80

retriever = PathwayRetriever(host=PATHWAY_HOST, port=PATHWAY_PORT)
```

### Chat Engine

We use `CondenseQuestionChatEngine` to create the RAG chatbot with LlamaIndex. One advantage of this chat engine is that it uses the context provided in the conversation history to write the search query. This results in answers that are more contextually relevant.

For further improvements to the pipeline, you can modify the chat engine type, prompt, and other parameters. For simplicity, we‚Äôll use the default settings.

```
chat_engine = CondensePlusContextChatEngine.from_defaults(
    retriever=retriever,
    verbose=True,
)
```

## 3. Creating the UI with Streamlit

Create a title for the app and initialize session state values for the chatbot‚Äôs message history and the chat engine.

```
st.title("Pathway + LlamaIndex")

if "messages" not in st.session_state.keys():
    st.session_state.messages = [
        {"role": "assistant", "content": "Hi, ask me a question. My knowledge is always up to date!"}
    ]

    st.session_state.chat_engine = chat_engine
```

Prompt the user for a question, store any user input in session state, and print messages from the user and the assistant.

```
if prompt := st.chat_input("Your question"):
    st.session_state.messages.append({"role": "user", "content": prompt})

for message in st.session_state.messages:
    with st.chat_message(message["role"]):
        st.write(message["content"])
```

If the last message is from the user and the assistant is preparing an answer, create a `st.spinner` widget. Add the message content and role to the message history.

```
if st.session_state.messages[-1]["role"] != "assistant":
    with st.chat_message("assistant"):
        with st.spinner("Thinking..."):
            response = st.session_state.chat_engine.chat(prompt)
            st.write(response.response)
            message = {"role": "assistant", "content": response.response}
            st.session_state.messages.append(message)
```

## Running the app

### On Streamlit Community Cloud

The demo is hosted on Streamlit Community Cloud [here](https://chat-realtime-sharepoint-gdrive.streamlit.app/?ref=streamlit.ghost.io). This version of the app uses Pathway's [hosted document pipelines](https://cloud.pathway.com/docindex?ref=streamlit.ghost.io).

### **On your local machine**

1. Clone [this repository](https://github.com/pathway-labs/realtime-indexer-qa-chat?ref=streamlit.ghost.io) locally.
2. Create a `.env` file under the root folder to store your OpenAI API key. This demo uses the OpenAI GPT model to answer questions.
3. You also need a Pathway instance for vector search. For local deployment, see the [vector store guide](https://pathway.com/developers/showcases/vectorstore_pipeline?ref=streamlit.ghost.io) and [Pathway Deployment](https://pathway.com/developers/user-guide/deployment/docker-deployment?ref=streamlit.ghost.io).
4. Run `streamlit run ui.py`.

Congrats! Now you‚Äôre ready to chat with your documents and any file updates will be reflected by your app in real-time, thanks to Pathway.

## Summing up

In this tutorial, you created and deployed a real-time RAG chatbot app. You also learned how easy it is to use Streamlit, LlamaIndex, and Pathway together, thanks to LlamaIndex‚Äôs Pathway Retriever. The end result is a RAG app that always has access to the most up-to-date version of your knowledge base.
